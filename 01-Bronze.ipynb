{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f535574b-3df1-4ba9-93d8-50476f75ad99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b93fc197-b869-4708-8884-9c907d77262b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remote blob path: wasbs://marketplace@bgupb202402juanbarriento.blob.core.windows.net/\n"
     ]
    }
   ],
   "source": [
    " # Azure storage access info\n",
    "blob_account_name = \"bgupb202402juanbarriento\"\n",
    "blob_container_name = \"marketplace\"\n",
    "blob_sas_token = 'sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-12-01T01:56:16Z&st=2024-10-16T17:56:16Z&spr=https&sig=cM7vK4vvrL9YH31ZYI%2BX%2BmssMUTdRieP%2BMGcVNPoagA%3D'\n",
    "\n",
    "# Allow SPARK to read from Blob remotely\n",
    "wasbs_path = f'wasbs://{blob_container_name}@{blob_account_name}.blob.core.windows.net/'\n",
    "spark.conf.set( f'fs.azure.sas.{blob_container_name}.{blob_account_name}.blob.core.windows.net' , blob_sas_token)\n",
    "print('Remote blob path: ' + wasbs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05531b1c-fd36-4dbd-949f-8c7956f9cb4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# #Raw Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4920fa7-957f-4c1e-8cba-d2074a349425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/bigdata202402/marketplace/jdbr/raw/2019-Dec.csv.gz</td><td>2019-Dec.csv.gz</td><td>2951549375</td><td>1728325030000</td></tr><tr><td>dbfs:/bigdata202402/marketplace/jdbr/raw/2019-Nov.csv.gz</td><td>2019-Nov.csv.gz</td><td>2890421023</td><td>1728325204000</td></tr><tr><td>dbfs:/bigdata202402/marketplace/jdbr/raw/2019-Oct.csv.gz</td><td>2019-Oct.csv.gz</td><td>1741928540</td><td>1728324857000</td></tr><tr><td>dbfs:/bigdata202402/marketplace/jdbr/raw/2020-Apr.csv.gz</td><td>2020-Apr.csv.gz</td><td>2929933389</td><td>1728326221000</td></tr><tr><td>dbfs:/bigdata202402/marketplace/jdbr/raw/2020-Feb.csv.gz</td><td>2020-Feb.csv.gz</td><td>2347087112</td><td>1728325984000</td></tr><tr><td>dbfs:/bigdata202402/marketplace/jdbr/raw/2020-Jan.csv.gz</td><td>2020-Jan.csv.gz</td><td>2390063923</td><td>1728325496000</td></tr><tr><td>dbfs:/bigdata202402/marketplace/jdbr/raw/2020-Mar.csv.gz</td><td>2020-Mar.csv.gz</td><td>2417362205</td><td>1728326101000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/bigdata202402/marketplace/jdbr/raw/2019-Dec.csv.gz",
         "2019-Dec.csv.gz",
         2951549375,
         1728325030000
        ],
        [
         "dbfs:/bigdata202402/marketplace/jdbr/raw/2019-Nov.csv.gz",
         "2019-Nov.csv.gz",
         2890421023,
         1728325204000
        ],
        [
         "dbfs:/bigdata202402/marketplace/jdbr/raw/2019-Oct.csv.gz",
         "2019-Oct.csv.gz",
         1741928540,
         1728324857000
        ],
        [
         "dbfs:/bigdata202402/marketplace/jdbr/raw/2020-Apr.csv.gz",
         "2020-Apr.csv.gz",
         2929933389,
         1728326221000
        ],
        [
         "dbfs:/bigdata202402/marketplace/jdbr/raw/2020-Feb.csv.gz",
         "2020-Feb.csv.gz",
         2347087112,
         1728325984000
        ],
        [
         "dbfs:/bigdata202402/marketplace/jdbr/raw/2020-Jan.csv.gz",
         "2020-Jan.csv.gz",
         2390063923,
         1728325496000
        ],
        [
         "dbfs:/bigdata202402/marketplace/jdbr/raw/2020-Mar.csv.gz",
         "2020-Mar.csv.gz",
         2417362205,
         1728326101000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls(\"dbfs:/bigdata202402/marketplace/jdbr/raw\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fc4a1fc-1070-4f67-a1a9-67e297008c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/bigdata202402/marketplace/jdbr/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f8ae0ff-8729-441f-a0a2-1bbaee0dd3c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af81a09c-d53c-4ab0-a509-70527c37d3ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_filtered = df_raw.filter((df_raw.category_code != 'null') & (df_raw.brand != 'null') & (df_raw.user_session != 'null'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f442da8b-d110-4e96-a6f3-430cd2fc33dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_filtered_categories = df_filtered.withColumn('category', split(col('category_code'), '\\.')\n",
    ".getItem(0)).withColumn('subcategory', split(col('category_code'), '\\.').getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ebe9278-d798-4d4b-9a52-f1b71108499b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_filtered_categories) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d0606ea-8795-43f6-8d4c-d75d25a77ee2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze = df_filtered_categories.withColumn(\n",
    "'price'\n",
    ", df_filtered_categories[\n",
    "'price'\n",
    "].cast(\n",
    "'double'\n",
    ")).withColumn(\n",
    "'month'\n",
    ", month(\n",
    "'event_time'\n",
    ")).withColumn(\n",
    "'year'\n",
    ",year(\n",
    "'event_time'\n",
    ")).withColumn(\n",
    "'event_time_timestamp'\n",
    ", to_timestamp(\n",
    "'event_time'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e140156-82b8-4b76-9b29-a5b516f859c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete any residual data from prior executions for an idempotent run\n",
    "dbutils.fs.rm(f\"{wasbs_path}/bronze\",recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0071e359-f1c4-41a2-aac4-cb948ccbdcdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_bronze.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"year\", \"month\").save(f\"{wasbs_path}/bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b87a5f81-2b50-4e35-85c6-f5d563def51f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "USE jdbr_mkp;\n",
    "DROP TABLE IF EXISTS mkp_bronze;\n",
    "CREATE TABLE mkp_bronze\n",
    "USING DELTA\n",
    "LOCATION 'wasbs://marketplace@bgupb202402juanbarriento.blob.core.windows.net/bronze/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b2509c7-ed7e-4de7-bd00-6af5751000c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2944297148578667>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m df_bronze\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myear\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmonth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mwasbs_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/bronze\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1732\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1730\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n",
       "\u001B[1;32m   1731\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:254\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    250\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    251\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    252\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    253\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 254\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    255\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    256\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: 8797d5f2-b0ed-4f60-8511-a5cfc0b179e0).\n",
       "To enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n",
       "'.option(\"mergeSchema\", \"true\")'.\n",
       "For other operations, set the session configuration\n",
       "spark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\n",
       "specific to the operation for details.\n",
       "\n",
       "Table schema:\n",
       "root\n",
       "-- event_time: string (nullable = true)\n",
       "-- event_type: string (nullable = true)\n",
       "-- product_id: string (nullable = true)\n",
       "-- category_id: string (nullable = true)\n",
       "-- category_code: string (nullable = true)\n",
       "-- brand: string (nullable = true)\n",
       "-- price: double (nullable = true)\n",
       "-- user_id: string (nullable = true)\n",
       "-- user_session: string (nullable = true)\n",
       "-- category: string (nullable = true)\n",
       "-- subcategory: string (nullable = true)\n",
       "-- month: integer (nullable = true)\n",
       "-- year: integer (nullable = true)\n",
       "-- even_time: timestamp (nullable = true)\n",
       "\n",
       "\n",
       "Data schema:\n",
       "root\n",
       "-- event_time: string (nullable = true)\n",
       "-- event_type: string (nullable = true)\n",
       "-- product_id: string (nullable = true)\n",
       "-- category_id: string (nullable = true)\n",
       "-- category_code: string (nullable = true)\n",
       "-- brand: string (nullable = true)\n",
       "-- price: double (nullable = true)\n",
       "-- user_id: string (nullable = true)\n",
       "-- user_session: string (nullable = true)\n",
       "-- category: string (nullable = true)\n",
       "-- subcategory: string (nullable = true)\n",
       "-- month: integer (nullable = true)\n",
       "-- year: integer (nullable = true)\n",
       "-- event_time_timestamp: timestamp (nullable = true)\n",
       "\n",
       "         \n",
       "To overwrite your schema or change partitioning, please set:\n",
       "'.option(\"overwriteSchema\", \"true\")'.\n",
       "\n",
       "Note that the schema can't be overwritten when using\n",
       "'replaceWhere'.\n",
       "         "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: 8797d5f2-b0ed-4f60-8511-a5cfc0b179e0).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- event_time: string (nullable = true)\n-- event_type: string (nullable = true)\n-- product_id: string (nullable = true)\n-- category_id: string (nullable = true)\n-- category_code: string (nullable = true)\n-- brand: string (nullable = true)\n-- price: double (nullable = true)\n-- user_id: string (nullable = true)\n-- user_session: string (nullable = true)\n-- category: string (nullable = true)\n-- subcategory: string (nullable = true)\n-- month: integer (nullable = true)\n-- year: integer (nullable = true)\n-- even_time: timestamp (nullable = true)\n\n\nData schema:\nroot\n-- event_time: string (nullable = true)\n-- event_type: string (nullable = true)\n-- product_id: string (nullable = true)\n-- category_id: string (nullable = true)\n-- category_code: string (nullable = true)\n-- brand: string (nullable = true)\n-- price: double (nullable = true)\n-- user_id: string (nullable = true)\n-- user_session: string (nullable = true)\n-- category: string (nullable = true)\n-- subcategory: string (nullable = true)\n-- month: integer (nullable = true)\n-- year: integer (nullable = true)\n-- event_time_timestamp: timestamp (nullable = true)\n\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         "
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "_LEGACY_ERROR_TEMP_DELTA_0007",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-2944297148578667>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df_bronze\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mpartitionBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myear\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmonth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mwasbs_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/bronze\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1732\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1730\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n\u001B[1;32m   1731\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:254\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    250\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    251\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    252\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    253\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 254\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    255\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    256\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: 8797d5f2-b0ed-4f60-8511-a5cfc0b179e0).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- event_time: string (nullable = true)\n-- event_type: string (nullable = true)\n-- product_id: string (nullable = true)\n-- category_id: string (nullable = true)\n-- category_code: string (nullable = true)\n-- brand: string (nullable = true)\n-- price: double (nullable = true)\n-- user_id: string (nullable = true)\n-- user_session: string (nullable = true)\n-- category: string (nullable = true)\n-- subcategory: string (nullable = true)\n-- month: integer (nullable = true)\n-- year: integer (nullable = true)\n-- even_time: timestamp (nullable = true)\n\n\nData schema:\nroot\n-- event_time: string (nullable = true)\n-- event_type: string (nullable = true)\n-- product_id: string (nullable = true)\n-- category_id: string (nullable = true)\n-- category_code: string (nullable = true)\n-- brand: string (nullable = true)\n-- price: double (nullable = true)\n-- user_id: string (nullable = true)\n-- user_session: string (nullable = true)\n-- category: string (nullable = true)\n-- subcategory: string (nullable = true)\n-- month: integer (nullable = true)\n-- year: integer (nullable = true)\n-- event_time_timestamp: timestamp (nullable = true)\n\n         \nTo overwrite your schema or change partitioning, please set:\n'.option(\"overwriteSchema\", \"true\")'.\n\nNote that the schema can't be overwritten when using\n'replaceWhere'.\n         "
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "USE jdbr_mkp;\n",
    "select * from mkp_bronze\n",
    "LIMIT 100;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b813b9cf-49cc-4553-be1d-7e00e14b7f83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bronze Layer"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3140036345287175,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01-Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}